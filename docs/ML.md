# 1. Basic Info
!!! info "一些概念"
      **监督学习(SL)**:
      在样本标签已知的情况下，可以统计出各类训练样本不同的描述量，如其概率分布，或在特征空间分布的区域等，利用这些参数进行分类器设计，称为有监督的学习方法。

      **无监督学习(USL)**:
      训练数据没有标签或者答案，目的是找出数据内部的关联和模式，趋势。

      **半监督学习(SSL)**:
      训练数据一部分数据为生成数据，一部分数据为监督数据，算法分为生成器和判定器两部分， 生成器的目标是使判定器接受自己的数据，判别器是为了最大可能的区分生成数据和监督数据。通过不断的训练使两者都达到最佳性能。

      **强化学习(RL)**:
      给予算法一个不断试错，并具有奖励机制的场景，最终使算法找到最佳路径或者策略。

- SL:决策树, 朴素贝叶斯, 逻辑回归, KNN, SVM, 神经网络, 随机森林, 遗传算法
- USL:聚类k-means, 层次聚类

# 2. CLuster Analysis

聚类分析是一种无监督(Unsupervised Learning)分类方法：数据集中的数据没有预定义的类别标号**（无训练集和训练的过程）**

聚类分析的目标是，簇内的对象相互之间是相似的，而不同簇中的对象是不同的。

在聚类分析中，样本之间的相似性通常采用样本之间的距离来表示 (1) 。{==根据特殊的数据特点定义不同的“距离”==}
{ .annotate }

1. 两个样本之间的距离越大，表示两个样本越不相似性，差异性越大；
   两个样本之间的距离越小，表示两个样本越相似性，差异性越小。
   特例：当两个样本之间的距离为零时，表示两个样本完全一样，无差异。

Common distance: Euclidean, Manhattan, Minkowski distance

## Common Cluster Algorithm
- 划分方法:
k-means算法（k-均值算法）
k-medoids算法（k-中心算法）
- 层次方法:
AGNES算法（合并聚类法）
DIANA算法（分裂聚类法）
- 基于密度:
DBSCAN，OPTICS
- 基于网格:
CLIQUE,STING
### k-means alg.
就是通过预先给定一些中心点作为“质点”, 他们保持不变, 之后进行聚类, 最终用total within-sum-of-squares (total WSS)(总体类内误差平方和)最小约束, 形成不交叠的数据集.
!!! note "优劣势"
      优点：
      可扩展性较好，算法复杂度为O(nkt)。
      其中：n为样本个数，k是簇的个数，t是迭代次数。
      
      缺点：
      对于任意一个数据集，k-means算法无法达到全局最优，只能达到局部最优。
      簇数目k需要事先给定，但非常难以选定；
      初始聚类中心的选择对聚类结果有较大的影响；
      不适合于发现非球状簇；
      对噪声和离群点数据敏感。

~~~ R title="kmeans.r"
kmeans(x, centers, iter.max = 10, nstart = 1,algorithm = c("Hartigan-Wong", "Lloyd", "Forgy","MacQueen"), trace=FALSE)
~~~

待聚类样本组织在x指定的矩阵或数据框中。

参数centers：若为一个整数，则表示聚类数目K；若为一个矩阵（行数等于聚类数目K，列数等于聚类变量个数），则表示初始类质心，每一行表示一个初始类质心。
参数iter.max用于指定最大迭代次数，默认为10次。R中仅以最大迭代次数作为终止迭代条件。

当参数为centers为一个整数时，R将采用随机选择法从数据中抽取K个观测值作为初始类质心。不同的初始类质心对最终的聚类结果是存在影响的。所以，R为克服大数据集下终止迭代次数不充分大时，初始类质心抽取的随机性对聚类结果的影响，可指定参数nstart为一个大于1的值（默认为1），表示重复多次抽取质心。

!!! note "Kmeans函数的返回结果是一个列表"
      cluster：存储各观测所属的类别编号，也称聚类解。
      centers：存储各个类的最终类质心。
      totss：所有聚类变量的离差平方和之和，是对类内部观测数据点离散程度的测度。
      cotwithss：每个类内所有聚类变量的离差平方和之和的总和。
      betweenss：各类别间的聚类变量离差平方和之和
      size：各类的样本量。


